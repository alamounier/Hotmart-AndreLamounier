{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407e8f14-389c-4016-a2f5-92607081d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f\"Silver-Purchase-Extra-Info-{ts}\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df14a7de-ee74-44a6-afaf-191cadc82dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, to_utc_timestamp, col\n",
    "from pyspark.sql.types import DecimalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff73562-81d9-4aa0-9c2c-26a9d56a35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first execution !!!\n",
    "\n",
    "df_bronze = spark.read.parquet(\"data_lake/bronze/purchase_extra_info\")\n",
    "df_bronze.createOrReplaceTempView(\"purchase_extra_info_bronze\")\n",
    "\n",
    "df_purchase_extra_info_silver = spark.sql(\"\"\"\n",
    "WITH \n",
    "base AS (\n",
    "    SELECT \n",
    "        a.transaction_datetime,\n",
    "        a.transaction_date,\n",
    "        a.purchase_id,\n",
    "        a.subsidiary,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY a.purchase_id, a.transaction_datetime\n",
    "            ORDER BY a.transaction_datetime DESC\n",
    "        ) AS rn,\n",
    "        to_utc_timestamp(current_timestamp(), 'UTC') AS line_created_at,\n",
    "        a.ingestion_date AS bronze_ingestion_date\n",
    "    FROM purchase_extra_info_bronze a\n",
    ")\n",
    "SELECT \n",
    "    transaction_datetime,\n",
    "    transaction_date,\n",
    "    purchase_id,\n",
    "    subsidiary,\n",
    "    line_created_at,\n",
    "    bronze_ingestion_date\n",
    "FROM base\n",
    "WHERE rn = 1\n",
    "\"\"\")\n",
    "\n",
    "# Salvar como Delta Table\n",
    "df_purchase_extra_info_silver.coalesce(1).write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"transaction_date\") \\\n",
    "    .save(\"data_lake/silver/purchase_extra_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0d720cb-49d2-4463-ac8c-af3fa5e99c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    max as max_,\n",
    "    row_number,\n",
    "    current_timestamp,\n",
    "    to_utc_timestamp\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "SILVER_PATH = \"data_lake/silver/purchase_extra_info\"\n",
    "BRONZE_PATH = \"data_lake/bronze/purchase_extra_info\"\n",
    "\n",
    "# =====================================================\n",
    "# 1. Ler Silver atual (se existir)\n",
    "# =====================================================\n",
    "try:\n",
    "    df_silver_current = spark.read.parquet(SILVER_PATH)\n",
    "except AnalysisException:\n",
    "    df_silver_current = None\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2. Última ingestion processada\n",
    "# =====================================================\n",
    "last_ingestion = (\n",
    "    df_silver_current\n",
    "    .agg(max_(\"bronze_ingestion_date\").alias(\"max_date\"))\n",
    "    .first()[\"max_date\"]\n",
    ") if df_silver_current is not None else None\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. Ler Bronze (incremental ou full)\n",
    "# =====================================================\n",
    "df_bronze = spark.read.parquet(BRONZE_PATH)\n",
    "\n",
    "df_bronze_incremental = (\n",
    "    df_bronze.filter(col(\"ingestion_date\") > last_ingestion)\n",
    "    if last_ingestion\n",
    "    else df_bronze\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4. Transformações Bronze → Silver\n",
    "# =====================================================\n",
    "df_bronze_ready = (\n",
    "    df_bronze_incremental\n",
    "    .withColumn(\n",
    "        \"line_created_at\",\n",
    "        to_utc_timestamp(current_timestamp(), \"UTC\")\n",
    "    )\n",
    "    .withColumnRenamed(\n",
    "        \"ingestion_date\",\n",
    "        \"bronze_ingestion_date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5. Union Silver + Bronze incremental\n",
    "# =====================================================\n",
    "df_union = (\n",
    "    df_silver_current.drop(\"rn\").unionByName(df_bronze_ready)\n",
    "    if df_silver_current is not None\n",
    "    else df_bronze_ready\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 6. Deduplicação por evento (CDC)\n",
    "# =====================================================\n",
    "event_window = Window.partitionBy(\n",
    "    \"purchase_id\",\n",
    "    \"transaction_datetime\"\n",
    ").orderBy(\n",
    "    col(\"bronze_ingestion_date\").desc()\n",
    ")\n",
    "\n",
    "df_dedup = (\n",
    "    df_union\n",
    "    .withColumn(\"rn\", row_number().over(event_window))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 7. Flag de último estado por purchase\n",
    "# =====================================================\n",
    "purchase_window = Window.partitionBy(\n",
    "    \"purchase_id\"\n",
    ").orderBy(\n",
    "    col(\"transaction_datetime\").desc()\n",
    ")\n",
    "\n",
    "df_purchase_extra_info_silver_final = (\n",
    "    df_dedup\n",
    "    .withColumn(\"rk\", row_number().over(purchase_window))\n",
    "    .withColumn(\"is_latest\", col(\"rk\") == 1)\n",
    "    .drop(\"rk\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d754678-d8c7-4500-93b8-66ff2eee7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 7. Escrita final da Silver\n",
    "# =====================================================\n",
    "df_purchase_extra_info_silver_final.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"transaction_date\") \\\n",
    "    .save(\"data_lake/silver/purchase_extra_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46efb89b-c5b8-46bc-ae3c-c9f2c7b12e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------+--------------------+---------------------+---------+----------------+\n",
      "|transaction_datetime|purchase_id|   subsidiary|     line_created_at|bronze_ingestion_date|is_latest|transaction_date|\n",
      "+--------------------+-----------+-------------+--------------------+---------------------+---------+----------------+\n",
      "| 2023-03-12 07:00:00|         69|internacional|2026-01-14 23:18:...| 2026-01-14 23:11:...|     true|      2023-03-12|\n",
      "| 2023-01-25 23:59:59|         56|internacional|2026-01-14 23:18:...| 2026-01-14 23:11:...|     true|      2023-01-25|\n",
      "| 2023-01-23 00:05:00|         55|     nacional|2026-01-14 23:18:...| 2026-01-14 23:11:...|     true|      2023-01-23|\n",
      "| 2023-02-28 01:10:00|         69|     nacional|2026-01-14 23:18:...| 2026-01-14 23:11:...|    false|      2023-02-28|\n",
      "+--------------------+-----------+-------------+--------------------+---------------------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criar view temporária\n",
    "df_silver_final = spark.read.parquet(\"data_lake/silver/purchase_extra_info\")\n",
    "df_silver_final.createOrReplaceTempView(\"purchase_extra_info_silver\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM purchase_extra_info_silver\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
