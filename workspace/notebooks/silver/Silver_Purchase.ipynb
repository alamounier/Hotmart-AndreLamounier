{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a2044d4-6e43-41a4-81b3-9bbd65d3312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f\"Silver-Purchase-{ts}\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4059fb62-804c-49e5-a916-c2fe8d2afae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, to_utc_timestamp, col\n",
    "from pyspark.sql.types import DecimalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a6c69f4-65f2-41a9-b20e-165538264e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first execution \n",
    "\n",
    "df_bronze = spark.read.parquet(\"data_lake/bronze/purchase\")\n",
    "df_bronze.createOrReplaceTempView(\"purchase_bronze\")\n",
    "\n",
    "df_purchase_silver = spark.sql(\"\"\"\n",
    "WITH \n",
    "base AS (\n",
    "    SELECT \n",
    "        a.transaction_datetime,\n",
    "        a.transaction_date,\n",
    "        a.purchase_id,\n",
    "        a.buyer_id,\n",
    "        a.prod_item_id,\n",
    "        a.order_date,\n",
    "        a.release_date,\n",
    "        a.producer_id,\n",
    "        CASE \n",
    "            WHEN a.release_date IS NOT NULL THEN 'Invoiced'\n",
    "            ELSE 'Not Invoiced'\n",
    "        END AS invoiced_status,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY a.purchase_id, a.transaction_datetime\n",
    "            ORDER BY a.transaction_datetime DESC\n",
    "        ) AS rn,\n",
    "        to_utc_timestamp(current_timestamp(), 'UTC') AS line_created_at,\n",
    "        a.ingestion_date AS bronze_ingestion_date\n",
    "    FROM purchase_bronze a\n",
    ")\n",
    "SELECT *\n",
    "FROM base\n",
    "WHERE rn = 1\n",
    "\"\"\")\n",
    "\n",
    "# Salvar como Delta Table\n",
    "df_purchase_silver.coalesce(1).write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"transaction_date\") \\\n",
    "    .save(\"data_lake/silver/purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c796715d-f755-4ca9-8713-89c5fd625cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    max as max_,\n",
    "    row_number,\n",
    "    when,\n",
    "    current_timestamp,\n",
    "    to_utc_timestamp\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "SILVER_PATH = \"data_lake/silver/purchase\"\n",
    "BRONZE_PATH = \"data_lake/bronze/purchase\"\n",
    "\n",
    "# =====================================================\n",
    "# 1. Ler Silver atual (se existir)\n",
    "# =====================================================\n",
    "try:\n",
    "    df_silver_current = spark.read.parquet(SILVER_PATH)\n",
    "except AnalysisException:\n",
    "    df_silver_current = None\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2. Última ingestion processada\n",
    "# =====================================================\n",
    "last_ingestion = (\n",
    "    df_silver_current\n",
    "    .agg(max_(\"bronze_ingestion_date\").alias(\"max_date\"))\n",
    "    .first()[\"max_date\"]\n",
    ") if df_silver_current is not None else None\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. Ler Bronze (incremental ou full)\n",
    "# =====================================================\n",
    "df_bronze = spark.read.parquet(BRONZE_PATH)\n",
    "\n",
    "df_bronze_incremental = (\n",
    "    df_bronze.filter(col(\"ingestion_date\") > last_ingestion)\n",
    "    if last_ingestion\n",
    "    else df_bronze\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4. Transformações Bronze → Silver\n",
    "# =====================================================\n",
    "df_bronze_ready = (\n",
    "    df_bronze_incremental\n",
    "    .withColumn(\n",
    "        \"invoiced_status\",\n",
    "        when(col(\"release_date\").isNotNull(), \"Invoiced\")\n",
    "        .otherwise(\"Not Invoiced\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"line_created_at\",\n",
    "        to_utc_timestamp(current_timestamp(), \"UTC\")\n",
    "    )\n",
    "    .withColumnRenamed(\n",
    "        \"ingestion_date\",\n",
    "        \"bronze_ingestion_date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5. Union Silver + Bronze incremental\n",
    "# =====================================================\n",
    "df_union = (\n",
    "    df_silver_current.drop(\"rn\").unionByName(df_bronze_ready)\n",
    "    if df_silver_current is not None\n",
    "    else df_bronze_ready\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 6. Deduplicação por evento (CDC)\n",
    "# =====================================================\n",
    "event_window = Window.partitionBy(\n",
    "    \"purchase_id\",\n",
    "    \"transaction_datetime\"\n",
    ").orderBy(\n",
    "    col(\"bronze_ingestion_date\").desc()\n",
    ")\n",
    "\n",
    "df_silver_dedup = (\n",
    "    df_union\n",
    "    .withColumn(\"rn\", row_number().over(event_window))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 7. Último estado por purchase\n",
    "# =====================================================\n",
    "purchase_window = Window.partitionBy(\n",
    "    \"purchase_id\"\n",
    ").orderBy(\n",
    "    col(\"transaction_datetime\").desc()\n",
    ")\n",
    "\n",
    "df_silver_final = (\n",
    "    df_silver_dedup\n",
    "    .withColumn(\"rk\", row_number().over(purchase_window))\n",
    "    .withColumn(\n",
    "        \"is_latest\",\n",
    "        col(\"rk\") == 1\n",
    "    )\n",
    "    .drop(\"rk\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b78e394-2078-4135-a687-53a9ef0c0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 7. Escrita final da Silver\n",
    "# =====================================================\n",
    "df_silver_final.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"transaction_date\") \\\n",
    "    .save(\"data_lake/silver/purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4df45ec-9b3d-4e99-96fd-7fae8fffaa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+------------+----------+------------+-----------+---------------+--------------------+---------------------+---------+----------------+\n",
      "|transaction_datetime|purchase_id|buyer_id|prod_item_id|order_date|release_date|producer_id|invoiced_status|     line_created_at|bronze_ingestion_date|is_latest|transaction_date|\n",
      "+--------------------+-----------+--------+------------+----------+------------+-----------+---------------+--------------------+---------------------+---------+----------------+\n",
      "| 2023-07-15 09:00:00|         55|  160001|           5|2023-01-20|  2023-03-01|     852852|       Invoiced|2026-01-14 22:33:...| 2026-01-14 16:21:...|     true|      2023-07-15|\n",
      "| 2023-02-26 03:00:00|         69|  160001|          18|2023-01-26|  2023-02-28|      96967|       Invoiced|2026-01-14 22:33:...| 2026-01-14 16:21:...|     true|      2023-02-26|\n",
      "| 2023-02-05 10:00:00|         55|  160001|           5|2023-01-20|  2023-01-20|     852852|       Invoiced|2026-01-14 22:33:...| 2026-01-14 16:21:...|    false|      2023-02-05|\n",
      "| 2023-01-20 22:00:00|         55|   15947|           5|2023-01-20|  2023-01-20|     852852|       Invoiced|2026-01-14 22:33:...| 2026-01-14 16:21:...|    false|      2023-01-20|\n",
      "| 2023-01-26 00:01:00|         56|  369798|      746520|2023-01-25|        null|     963963|   Not Invoiced|2026-01-14 22:33:...| 2026-01-14 16:21:...|     true|      2023-01-26|\n",
      "+--------------------+-----------+--------+------------+----------+------------+-----------+---------------+--------------------+---------------------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criar view temporária\n",
    "df_silver_final = spark.read.parquet(\"data_lake/silver/purchase\")\n",
    "df_silver_final.createOrReplaceTempView(\"purchase_silver\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM purchase_silver\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9b40a-58c9-43ce-9594-226daa73540f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
